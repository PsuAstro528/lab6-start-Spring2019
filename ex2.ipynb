{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astro 528, Lab 6, Exercise 2\n",
    "## Parallelization for Cluster using Distributed-Memory Model\n",
    "\n",
    "In this lab exercise, we'll perform the same calculations as in exercise 2 of lab 5.  However, instead of using a multi-core workstation, we will [run the calculations on the ICS ACI-B cluster](https://ics.psu.edu/computing-services/ics-aci-user-guide/#07-00-running-jobs-on-aci-b) using a distributed memory model.  \n",
    "\n",
    "You're welcome to run this notebook one cell at a time as in other labs to see how it works.  However, the main point of this lab is to see how to run such a calculation in parallel over multiple processor cores that are not necessarily on the same processor.  Therefore, you'll use the [command line interface](https://ics.psu.edu/computing-services/ics-aci-user-guide/#05-00-basics-aci-resources) to submit the jobs [ex2.pbs](https://github.com/PsuAstro528/lab6-start/blob/master/ex2.pbs).  Follow the instructions in the [lab's README](https://github.com/PsuAstro528/lab6-start/blob/master/README.md)\n",
    "\n",
    "First, make sure you have the necessary packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:17:42.438000-05:00",
     "start_time": "2019-02-25T07:17:40.141Z"
    }
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "# Shouldn't need to instantiate the project, since \n",
    "# you'll have already run as part of exercise 1.\n",
    "# Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, packages are installed to disk, so you only want to run `instantiate` on the master node.\n",
    "\n",
    "### Getting setup for parallel computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the CSV and DataFrames packages that only need to be loaded by the master process.\n",
    "Then we'll load Julia's `Distributed` module that provides much of the needed functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:17:49.984000-05:00",
     "start_time": "2019-02-25T07:17:40.143Z"
    }
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "using Distributed\n",
    "println(\"# Julia is using \", nworkers(), \" workers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:17:55.974000-05:00",
     "start_time": "2019-02-25T07:17:40.145Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomments the line below, if you're running manually in a notebook and\n",
    "# want to test with multiple processors.\n",
    "# addprocs(4); println(\"# Now Julia is using \", nworkers(), \" workers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Packages and modules\n",
    "\n",
    "Now, we can start loading other packages that we'll be using.  Since we want the packages to be in scope on each worker, then we need to add the @everywhere macro in front of the using statement.  Before we can do that we need to activate the project on each of the workers, so they know that they can use the packages in Project.toml.  We don't need to install or instantiate on each worker, since those write files to disk and all the workers have access to the same filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:17:58.903000-05:00",
     "start_time": "2019-02-25T07:17:40.148Z"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere using Pkg\n",
    "@everywhere Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:01.598000-05:00",
     "start_time": "2019-02-25T07:17:40.150Z"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere using Distributions\n",
    "@everywhere using DistributedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, I've written several functions that will be used to generate simulated spectra.  This serves a couple of purposes.\n",
    "First, you'll use the code in the exercise, so you have a calculation that's big enough to be worth parallelizing.  For the purposes of this exercise, it's not essential that you review the code I provided in `.jl` files.  However, the second purpose of providing this is to demonstrate several of the programming patterns that we've discussed in class.  For example, the code in the `ModelSpectrum` module\n",
    "- is in the form of several small functions, each which does one specific task.  \n",
    "- has been moved out of the Jupyter notebook and into `.jl` files in the `src` directory.\n",
    "- creates objects to represent spectra and a convolution kernel.\n",
    "- uses [abstract types](https://docs.julialang.org/en/v1/manual/types/#Abstract-Types-1) and [parametric types](https://docs.julialang.org/en/v1/manual/types/#Parametric-Types-1), so as to create type-stable functions. \n",
    "- has been  put into a Julia [module](https://docs.julialang.org/en/v1/manual/modules/index.html), so that it can be easily loaded and so as to limit potential for namespace conflicts.\n",
    "\n",
    "You don't need to read all of this code right now.  But, when you're writing code for your class project, you're likely to want to make use of some of these same programming patterns. So, it may be useful to refer back to this code later to help see examples of how to apply these design patterns in practice.  \n",
    "        \n",
    "For now, let's include just the file that has the code for the `ModelSpectrum` module.  `src/ModelSpectrum.jl` includes the code from the other files, `spectrum.jl` and `convolution_kernels.jl`.  We'll preface it with `@everywhere`, since we want all of the processors to be able to make use of these function and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:03.022000-05:00",
     "start_time": "2019-02-25T07:17:40.153Z"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere include(\"src/ModelSpectrum.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll bring that module into scope.  Note that since this is not a package, we need to include a `.` to tell Julia that it can the module in the current namespace, rather than needing to load a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:03.023000-05:00",
     "start_time": "2019-02-25T07:17:40.155Z"
    }
   },
   "outputs": [],
   "source": [
    "using .ModelSpectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize data to be analyzed\n",
    "In this exercise, we're going to create a model spectrum consisting of continuum, stellar absorption lines, telluric absorption lines.  \n",
    "The `ModelSpectrum` module provides a `SimulatedSpectrum` type, but we'll need to initialize a variable with some specific parameter values.  The function does that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:03.506000-05:00",
     "start_time": "2019-02-25T07:17:40.157Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Create an object that provides a model for the raw spetrum (i.e., before entering the telescope)\"\n",
    "function make_spectrum_object(;lambda_min = 4500, lambda_max = 7500, flux_scale = 1.0,\n",
    "        num_star_lines = 200, num_telluric_lines = 100, limit_line_effect = 10.0)\n",
    "\n",
    "    continuum_param = flux_scale .* [1.0, 1e-5, -2e-8]\n",
    "    \n",
    "    star_line_locs = rand(Uniform(lambda_min,lambda_max),num_star_lines)\n",
    "    star_line_widths = fill(1.0,num_star_lines)\n",
    "    star_line_depths = rand(Uniform(0,1.0),num_star_lines)\n",
    "    \n",
    "    telluric_line_locs = rand(Uniform(lambda_min,lambda_max),num_telluric_lines)\n",
    "    telluric_line_widths = fill(0.2,num_telluric_lines)\n",
    "    telluric_line_depths = rand(Uniform(0,0.4),num_telluric_lines)\n",
    "\n",
    "    SimulatedSpectrum(star_line_locs,star_line_widths,star_line_depths,telluric_line_locs,telluric_line_widths,telluric_line_depths,continuum_param=continuum_param,lambda_mid=0.5*(lambda_min+lambda_max),limit_line_effect=limit_line_effect)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we: \n",
    "1. create a set of wavelengths to observe the spectrum at, \n",
    "2. call the function above to create a spectrum object, \n",
    "3. create an object containing a model for the point spread function, and \n",
    "4. create an object that can compute the convolution of our spectral model with the point spread function model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:05.032000-05:00",
     "start_time": "2019-02-25T07:17:40.159Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.  Pick range of of wavelength to work on.\n",
    "lambda_min = 4500\n",
    "lambda_max = 7500\n",
    "# You may want to adjust the num_lambda to make things more/less computationally intensive\n",
    "num_lambda = 128*1024\n",
    "lambdas = collect(range(lambda_min,stop=lambda_max, length=num_lambda));\n",
    "\n",
    "# 2.  Create a model  spectrum that we'll analyze below\n",
    "raw_spectrum = make_spectrum_object(lambda_min=lambda_min,lambda_max=lambda_max)\n",
    "\n",
    "# 3.  Create a model for the point spread function (PSF)\n",
    "psf_widths  = [0.5, 1.0, 2.0]\n",
    "psf_weights = [0.8, 0.15, 0.05]\n",
    "psf_model = GaussianMixtureConvolutionKernel(psf_widths,psf_weights)\n",
    "\n",
    "# 4. Create a model for the the convolution of thte raw spectrum with the PDF model\n",
    "conv_spectrum = ConvolvedSpectrum(raw_spectrum,psf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking serial code\n",
    "\n",
    "*If the current job has just one worker process,* then we'll benchmark the calculation of this spectrum on a single processor.  Based on performance results from lab 5, we'll make use of Julia's [dot syntax](https://docs.julialang.org/en/v1/manual/functions/#man-vectorized-1) to [\"broadcast\" and \"fuse\"](https://docs.julialang.org/en/v1/base/arrays/#Broadcast-and-vectorization-1) the array operation.    We'll run it just a few times (rather than using `@btime`).\n",
    "Since the rest of the notebook is meant for distributed computing, we'll tell it to exit here if there's only a single worker.  Before we do, we'll flush the buffer, so any messages are written to `STDOUT` before the kernel exits.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:05.217000-05:00",
     "start_time": "2019-02-25T07:17:40.160Z"
    }
   },
   "outputs": [],
   "source": [
    "num_runs = 3\n",
    "if nworkers() == 1    \n",
    "    for i in 1:num_runs @time conv_spectrum.(lambdas); end\n",
    "    flush(stdout) # flush buffer before the kernel exits\n",
    "    sleep(1)      # wait for one second just to make sure\n",
    "    exit(0)       # Exit the script since completed serial calculation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this in a Jupyter notebook and got a message about the kernel dieing, don't worry.  We used the `exit` command to cause the julia kernel to exit.  If you're in the notebook server and want to keep going, then you can rerun the previous cells above, and either add some processors (e.g., `addprocs(4)`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Arrays\n",
    "Here, we want to spread the work over processor cores than are not necessarily on the same node, so we need to use a [distributed memory system](https://en.wikipedia.org/wiki/Distributed_memory).  This would be necessary if you wanted your job to run on more cores than are available on a single node.  It could also be useful if you wanted to increase the chances that the scheduler starts your job more quickly, since asking for several processors cores that don't need to be on the same node is easier to accommodate than asking for the same number of cores on a single node.  Here we'll use Julia's [DistributedArrays.jl](https://juliaparallel.github.io/DistributedArrays.jl/latest/index.html) package to make programming for distributed memory systems relatively easy.  \n",
    "\n",
    "Here we'll create a distributed array by simply applying `distribute` to our existing array of wavelengths.  (Remember that we could initialize a `DArray` more efficiently be letting each workers initializes its own data.  For convenience functions like `dzeros`, `dones`, `drand`, `drandn` and `dfill` act similarly to their counterparts without a `d` prefix, but create DArrays instead of regular Arrays.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:07.340000-05:00",
     "start_time": "2019-02-25T07:17:40.163Z"
    }
   },
   "outputs": [],
   "source": [
    "lambdas_dist = distribute(lambdas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first time we call a function, it takes some extra time and memory to compile it.  So let's do that again, this time benchmarking the `distribute` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:07.393000-05:00",
     "start_time": "2019-02-25T07:17:40.165Z"
    }
   },
   "outputs": [],
   "source": [
    "println(\"# Timing calls to distribute.\")\n",
    "@time lambdas_dist = distribute(lambdas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will apply `map` to a `lambda_dist`, which is a `DArray`.  `map` will parallelize the calculation and return the results in a `DArray`.  Each worker operates on the subset of the array that is local to that worker process.  We will call `collect` the data in order to copy all the data back to to the master process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:09.082000-05:00",
     "start_time": "2019-02-25T07:17:40.167Z"
    }
   },
   "outputs": [],
   "source": [
    "println(\"# Timing calls to map(...).\")\n",
    "for i in 1:num_runs \n",
    "    @time map(conv_spectrum,lambdas_dist) \n",
    "end\n",
    "\n",
    "println(\"# Timing calls to collect(map(...)).\")\n",
    "for i in 1:num_runs \n",
    "    @time collect(map(conv_spectrum,lambdas_dist)) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying all that data back for the master process to access added a significant amount to the total time.\n",
    "Sometimes you don't actually need to bring all the data back to the master process.  For example, you might have several calculations that can be done, each leaving the data distributed across many workers, until the very end.\n",
    "Another common scenario is that you want to performing a task that can be phrased as a [`mapreduce`](https://en.wikipedia.org/wiki/MapReduce) programming pattern.  For example, imagine that we only wanted to compute the total flux over a filter band.  Then we could use code like the following to reduce the amount of communications overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:16.177000-05:00",
     "start_time": "2019-02-25T07:17:40.170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Send the value lambda_min to each of the workers\n",
    "for p in workers() remotecall_fetch(()->lambda_min, p); end\n",
    "\n",
    "# Define a function on each of the workers\n",
    "@everywhere is_in_filter_band(x) = (lambda_min < x < lambda_min+100) ? one(x) : zero(x)\n",
    "\n",
    "# Run mapreduce, summing the product of the convolved spectrum and the filter's weight at each wavelength\n",
    "mapreduce(x->is_in_filter_band(x)*conv_spectrum(x), +, lambdas_dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:32.528000-05:00",
     "start_time": "2019-02-25T07:17:40.172Z"
    }
   },
   "outputs": [],
   "source": [
    "println(\"# Timing calls to mapreduce.\")\n",
    "for i in 1:num_runs \n",
    "    @time mapreduce(x->is_in_filter_band(x)*conv_spectrum(x), +, lambdas_dist) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output (without interfering with output from other jobs)\n",
    "Below, we'll demonstrate how to write the results to a file, making sure that different jobs don't overwrite each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:49.937000-05:00",
     "start_time": "2019-02-25T07:17:40.174Z"
    }
   },
   "outputs": [],
   "source": [
    "results = collect(map(conv_spectrum,lambdas_dist)) \n",
    "\n",
    "output_filename = \"ex2_out.csv\"\n",
    "if haskey(ENV,\"PBS_JOBID\")         # see if PBS_JOBID is among environment variables\n",
    "  m = match(r\"^(\\d+\\[?\\d*\\]?)\\.\",ENV[\"PBS_JOBID\"])  # \n",
    "  if m != nothing               \n",
    "      jobid_num = m.captures[1]\n",
    "      output_filename = \"ex2_out_\" * jobid_num * \".csv\"\n",
    "  end\n",
    "end\n",
    "CSV.write(output_filename,DataFrame(lambda=lambdas,result=results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
